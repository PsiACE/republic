{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Republic \u00b6 Use LLM capabilities like regular Python components, with auditable execution traces by default. Republic is not a bigger framework. It is a small set of composable primitives: LLM : One entry point for chat, tools, stream, and embeddings. StructuredOutput : Key interfaces return value + error . Tape : Append-only records with anchor/handoff/context/query. ToolExecutor : Tool calls can be automatic or manual. 30-Second Preview \u00b6 from republic import LLM llm = LLM(model=\"openrouter:openrouter/free\", api_key=\"<API_KEY>\") out = llm.chat(\"Explain tape-first in one sentence.\", max_tokens=48) if out.error: print(out.error.kind, out.error.message) else: print(out.value) What You Get \u00b6 Smaller API surface with stronger control. Visible tool execution paths without hidden magic. Run/tape-level behavior tracing for debugging and audits. Both text streaming and event streaming for CLI, web, and workers. This project is derived from lightning-ai/litai and inspired by pydantic/pydantic-ai ; we hope you like them too.","title":"Home"},{"location":"#republic","text":"Use LLM capabilities like regular Python components, with auditable execution traces by default. Republic is not a bigger framework. It is a small set of composable primitives: LLM : One entry point for chat, tools, stream, and embeddings. StructuredOutput : Key interfaces return value + error . Tape : Append-only records with anchor/handoff/context/query. ToolExecutor : Tool calls can be automatic or manual.","title":"Republic"},{"location":"#30-second-preview","text":"from republic import LLM llm = LLM(model=\"openrouter:openrouter/free\", api_key=\"<API_KEY>\") out = llm.chat(\"Explain tape-first in one sentence.\", max_tokens=48) if out.error: print(out.error.kind, out.error.message) else: print(out.value)","title":"30-Second Preview"},{"location":"#what-you-get","text":"Smaller API surface with stronger control. Visible tool execution paths without hidden magic. Run/tape-level behavior tracing for debugging and audits. Both text streaming and event streaming for CLI, web, and workers. This project is derived from lightning-ai/litai and inspired by pydantic/pydantic-ai ; we hope you like them too.","title":"What You Get"},{"location":"philosophy/","text":"Design Philosophy \u00b6 Republic is not trying to take over application logic. It provides a predictable, replayable, and evolvable set of building blocks. 1. Slow is Fast \u00b6 Define data boundaries and execution traces first, then optimize without rework. 2. Plain Python First \u00b6 Use ordinary functions, branches, and tests to organize intelligent workflows. 3. Structured over Clever \u00b6 Use one structured return shape and stable error kinds so callers can make explicit decisions. 4. Tape as Evidence \u00b6 Each run has an evidence chain: input, output, tools, errors, usage, and events. 5. Small Surface, Strong Composition \u00b6 Keep the core interfaces small, but composable into complex workflows.","title":"Philosophy"},{"location":"philosophy/#design-philosophy","text":"Republic is not trying to take over application logic. It provides a predictable, replayable, and evolvable set of building blocks.","title":"Design Philosophy"},{"location":"philosophy/#1-slow-is-fast","text":"Define data boundaries and execution traces first, then optimize without rework.","title":"1. Slow is Fast"},{"location":"philosophy/#2-plain-python-first","text":"Use ordinary functions, branches, and tests to organize intelligent workflows.","title":"2. Plain Python First"},{"location":"philosophy/#3-structured-over-clever","text":"Use one structured return shape and stable error kinds so callers can make explicit decisions.","title":"3. Structured over Clever"},{"location":"philosophy/#4-tape-as-evidence","text":"Each run has an evidence chain: input, output, tools, errors, usage, and events.","title":"4. Tape as Evidence"},{"location":"philosophy/#5-small-surface-strong-composition","text":"Keep the core interfaces small, but composable into complex workflows.","title":"5. Small Surface, Strong Composition"},{"location":"quickstart/","text":"Quickstart \u00b6 Install \u00b6 pip install republic Step 1: Create an LLM \u00b6 Republic uses the provider:model format. from republic import LLM llm = LLM(model=\"openrouter:openrouter/free\", api_key=\"<API_KEY>\") Step 2: Send one request \u00b6 out = llm.chat(\"Write one short release note.\", max_tokens=48) if out.error: print(\"error:\", out.error.kind, out.error.message) else: print(\"text:\", out.value) Step 3: Add an auditable trace to the session \u00b6 tape organizes context around anchors by default, so start with one handoff . tape = llm.tape(\"release-notes\") tape.handoff(\"draft_v1\", state={\"owner\": \"assistant\"}) reply = tape.chat(\"Summarize the version changes in three bullets.\", system_prompt=\"Keep it concise.\") print(reply.value) Step 4: Handle failures and fallback \u00b6 llm = LLM( model=\"openai:gpt-4o-mini\", fallback_models=[\"openrouter:openrouter/free\"], max_retries=2, api_key={\"openai\": \"<OPENAI_KEY>\", \"openrouter\": \"<OPENROUTER_KEY>\"}, ) result = llm.chat(\"say hello\", max_tokens=8) if result.error: # error.kind is one of invalid_input/config/provider/tool/temporary/not_found/unknown print(result.error.kind, result.error.message)","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#install","text":"pip install republic","title":"Install"},{"location":"quickstart/#step-1-create-an-llm","text":"Republic uses the provider:model format. from republic import LLM llm = LLM(model=\"openrouter:openrouter/free\", api_key=\"<API_KEY>\")","title":"Step 1: Create an LLM"},{"location":"quickstart/#step-2-send-one-request","text":"out = llm.chat(\"Write one short release note.\", max_tokens=48) if out.error: print(\"error:\", out.error.kind, out.error.message) else: print(\"text:\", out.value)","title":"Step 2: Send one request"},{"location":"quickstart/#step-3-add-an-auditable-trace-to-the-session","text":"tape organizes context around anchors by default, so start with one handoff . tape = llm.tape(\"release-notes\") tape.handoff(\"draft_v1\", state={\"owner\": \"assistant\"}) reply = tape.chat(\"Summarize the version changes in three bullets.\", system_prompt=\"Keep it concise.\") print(reply.value)","title":"Step 3: Add an auditable trace to the session"},{"location":"quickstart/#step-4-handle-failures-and-fallback","text":"llm = LLM( model=\"openai:gpt-4o-mini\", fallback_models=[\"openrouter:openrouter/free\"], max_retries=2, api_key={\"openai\": \"<OPENAI_KEY>\", \"openrouter\": \"<OPENROUTER_KEY>\"}, ) result = llm.chat(\"say hello\", max_tokens=8) if result.error: # error.kind is one of invalid_input/config/provider/tool/temporary/not_found/unknown print(result.error.kind, result.error.message)","title":"Step 4: Handle failures and fallback"},{"location":"guides/breaking-changes/","text":"Breaking Changes (2026-02-11) \u00b6 This document summarizes the breaking changes introduced in this branch and shows how to migrate with minimal effort. Summary \u00b6 Error handling is now fully aligned with standard Python style: Success paths return plain values. Failure paths raise ErrorPayload . StructuredOutput(value, error) is no longer used as a general return wrapper. If your code still depends on .value / .error , migrate using the patterns below. Scope \u00b6 1. StructuredOutput Removed \u00b6 StructuredOutput has been removed from core types and public exports. src/republic/core/results.py src/republic/core/__init__.py src/republic/__init__.py 2. Non-Streaming APIs Now Use \"Return Value + Exception\" \u00b6 The following APIs no longer return StructuredOutput : LLM.chat(...) -> str LLM.chat_async(...) -> str LLM.tool_calls(...) -> list[dict[str, Any]] LLM.tool_calls_async(...) -> list[dict[str, Any]] LLM.if_(...) -> bool LLM.if_async(...) -> bool LLM.classify(...) -> str LLM.classify_async(...) -> str LLM.embed(...) -> Any LLM.embed_async(...) -> Any Tape session shortcuts changed in the same way: Tape.chat(...) -> str Tape.chat_async(...) -> str Tape.tool_calls(...) -> list[dict[str, Any]] Tape.tool_calls_async(...) -> list[dict[str, Any]] 3. ToolExecutor.execute Error Semantics Changed \u00b6 ToolExecutor.execute(...) now raises ErrorPayload for invalid input, validation failures, missing context, unknown tools, and similar failures. It no longer returns a result object with an error field for these cases. 4. Tape Return-Type Simplification \u00b6 This branch also finalizes: ContextSelection removed. read_messages(...) now returns list[dict[str, Any]] . QueryResult removed. TapeQuery.all() now returns list[TapeEntry] , and errors are raised as ErrorPayload . read_entries() is deprecated. Use tape.query.all() for full entry reads. Migration Examples \u00b6 Chat \u00b6 Before: out = llm.chat(\"Ping\") if out.error: handle_error(out.error) else: print(out.value) After: from republic.core.results import ErrorPayload try: text = llm.chat(\"Ping\") print(text) except ErrorPayload as exc: handle_error(exc) Text Decision / Classify \u00b6 Before: decision = llm.if_(\"service down\", \"should page?\") if decision.error is None and decision.value: page_oncall() After: from republic.core.results import ErrorPayload try: decision = llm.if_(\"service down\", \"should page?\") if decision: page_oncall() except ErrorPayload as exc: handle_error(exc) Embedding \u00b6 Before: out = llm.embed(\"incident summary\") if out.error: handle_error(out.error) else: vectors = out.value After: from republic.core.results import ErrorPayload try: vectors = llm.embed(\"incident summary\") except ErrorPayload as exc: handle_error(exc) ToolExecutor \u00b6 Before: result = executor.execute(calls, tools=tools) if result.error: handle_error(result.error) else: print(result.tool_results) After: from republic.core.results import ErrorPayload try: result = executor.execute(calls, tools=tools) print(result.tool_results) except ErrorPayload as exc: handle_error(exc) Fast Detection \u00b6 Use this command to locate old calling patterns: rg -n \"StructuredOutput|\\\\.value\\\\b|\\\\.error\\\\b\" src tests Then migrate matches to \"return value + try/except ErrorPayload \". Release Guidance \u00b6 This is a clear API breaking change. Use a major version bump, or at minimum a minor bump with explicit release notes. If you maintain downstream SDK consumers, provide a migration note or codemod focused on replacing .value / .error branches.","title":"Breaking Changes"},{"location":"guides/breaking-changes/#breaking-changes-2026-02-11","text":"This document summarizes the breaking changes introduced in this branch and shows how to migrate with minimal effort.","title":"Breaking Changes (2026-02-11)"},{"location":"guides/breaking-changes/#summary","text":"Error handling is now fully aligned with standard Python style: Success paths return plain values. Failure paths raise ErrorPayload . StructuredOutput(value, error) is no longer used as a general return wrapper. If your code still depends on .value / .error , migrate using the patterns below.","title":"Summary"},{"location":"guides/breaking-changes/#scope","text":"","title":"Scope"},{"location":"guides/breaking-changes/#1-structuredoutput-removed","text":"StructuredOutput has been removed from core types and public exports. src/republic/core/results.py src/republic/core/__init__.py src/republic/__init__.py","title":"1. StructuredOutput Removed"},{"location":"guides/breaking-changes/#2-non-streaming-apis-now-use-return-value-exception","text":"The following APIs no longer return StructuredOutput : LLM.chat(...) -> str LLM.chat_async(...) -> str LLM.tool_calls(...) -> list[dict[str, Any]] LLM.tool_calls_async(...) -> list[dict[str, Any]] LLM.if_(...) -> bool LLM.if_async(...) -> bool LLM.classify(...) -> str LLM.classify_async(...) -> str LLM.embed(...) -> Any LLM.embed_async(...) -> Any Tape session shortcuts changed in the same way: Tape.chat(...) -> str Tape.chat_async(...) -> str Tape.tool_calls(...) -> list[dict[str, Any]] Tape.tool_calls_async(...) -> list[dict[str, Any]]","title":"2. Non-Streaming APIs Now Use \"Return Value + Exception\""},{"location":"guides/breaking-changes/#3-toolexecutorexecute-error-semantics-changed","text":"ToolExecutor.execute(...) now raises ErrorPayload for invalid input, validation failures, missing context, unknown tools, and similar failures. It no longer returns a result object with an error field for these cases.","title":"3. ToolExecutor.execute Error Semantics Changed"},{"location":"guides/breaking-changes/#4-tape-return-type-simplification","text":"This branch also finalizes: ContextSelection removed. read_messages(...) now returns list[dict[str, Any]] . QueryResult removed. TapeQuery.all() now returns list[TapeEntry] , and errors are raised as ErrorPayload . read_entries() is deprecated. Use tape.query.all() for full entry reads.","title":"4. Tape Return-Type Simplification"},{"location":"guides/breaking-changes/#migration-examples","text":"","title":"Migration Examples"},{"location":"guides/breaking-changes/#chat","text":"Before: out = llm.chat(\"Ping\") if out.error: handle_error(out.error) else: print(out.value) After: from republic.core.results import ErrorPayload try: text = llm.chat(\"Ping\") print(text) except ErrorPayload as exc: handle_error(exc)","title":"Chat"},{"location":"guides/breaking-changes/#text-decision-classify","text":"Before: decision = llm.if_(\"service down\", \"should page?\") if decision.error is None and decision.value: page_oncall() After: from republic.core.results import ErrorPayload try: decision = llm.if_(\"service down\", \"should page?\") if decision: page_oncall() except ErrorPayload as exc: handle_error(exc)","title":"Text Decision / Classify"},{"location":"guides/breaking-changes/#embedding","text":"Before: out = llm.embed(\"incident summary\") if out.error: handle_error(out.error) else: vectors = out.value After: from republic.core.results import ErrorPayload try: vectors = llm.embed(\"incident summary\") except ErrorPayload as exc: handle_error(exc)","title":"Embedding"},{"location":"guides/breaking-changes/#toolexecutor","text":"Before: result = executor.execute(calls, tools=tools) if result.error: handle_error(result.error) else: print(result.tool_results) After: from republic.core.results import ErrorPayload try: result = executor.execute(calls, tools=tools) print(result.tool_results) except ErrorPayload as exc: handle_error(exc)","title":"ToolExecutor"},{"location":"guides/breaking-changes/#fast-detection","text":"Use this command to locate old calling patterns: rg -n \"StructuredOutput|\\\\.value\\\\b|\\\\.error\\\\b\" src tests Then migrate matches to \"return value + try/except ErrorPayload \".","title":"Fast Detection"},{"location":"guides/breaking-changes/#release-guidance","text":"This is a clear API breaking change. Use a major version bump, or at minimum a minor bump with explicit release notes. If you maintain downstream SDK consumers, provide a migration note or codemod focused on replacing .value / .error branches.","title":"Release Guidance"},{"location":"guides/chat/","text":"Chat \u00b6 llm.chat(...) is the smallest entry point and fits most text-only workloads. Prompt Mode \u00b6 from republic import LLM llm = LLM(model=\"openrouter:openrouter/free\", api_key=\"<API_KEY>\") out = llm.chat(\"Output exactly one word: ready\", max_tokens=8) print(out.value, out.error) Messages Mode \u00b6 messages = [ {\"role\": \"system\", \"content\": \"Be concise.\"}, {\"role\": \"user\", \"content\": \"Explain tape-first in one sentence.\"}, ] out = llm.chat(messages=messages, max_tokens=48) Structured Error Handling \u00b6 result = llm.chat(\"Write one sentence.\") if result.error: if result.error.kind == \"temporary\": print(\"retry later\") else: print(\"fail fast:\", result.error.message) Retries and Fallback \u00b6 llm = LLM( model=\"openai:gpt-4o-mini\", fallback_models=[\"anthropic:claude-3-5-sonnet-latest\"], max_retries=3, api_key={\"openai\": \"<OPENAI_KEY>\", \"anthropic\": \"<ANTHROPIC_KEY>\"}, ) out = llm.chat(\"Give me one deployment checklist item.\") Recommendation: keep max_retries small (for example 2-4), and pick fallback models that are slightly more stable while still meeting quality requirements.","title":"Chat"},{"location":"guides/chat/#chat","text":"llm.chat(...) is the smallest entry point and fits most text-only workloads.","title":"Chat"},{"location":"guides/chat/#prompt-mode","text":"from republic import LLM llm = LLM(model=\"openrouter:openrouter/free\", api_key=\"<API_KEY>\") out = llm.chat(\"Output exactly one word: ready\", max_tokens=8) print(out.value, out.error)","title":"Prompt Mode"},{"location":"guides/chat/#messages-mode","text":"messages = [ {\"role\": \"system\", \"content\": \"Be concise.\"}, {\"role\": \"user\", \"content\": \"Explain tape-first in one sentence.\"}, ] out = llm.chat(messages=messages, max_tokens=48)","title":"Messages Mode"},{"location":"guides/chat/#structured-error-handling","text":"result = llm.chat(\"Write one sentence.\") if result.error: if result.error.kind == \"temporary\": print(\"retry later\") else: print(\"fail fast:\", result.error.message)","title":"Structured Error Handling"},{"location":"guides/chat/#retries-and-fallback","text":"llm = LLM( model=\"openai:gpt-4o-mini\", fallback_models=[\"anthropic:claude-3-5-sonnet-latest\"], max_retries=3, api_key={\"openai\": \"<OPENAI_KEY>\", \"anthropic\": \"<ANTHROPIC_KEY>\"}, ) out = llm.chat(\"Give me one deployment checklist item.\") Recommendation: keep max_retries small (for example 2-4), and pick fallback models that are slightly more stable while still meeting quality requirements.","title":"Retries and Fallback"},{"location":"guides/embeddings/","text":"Embeddings \u00b6 The embedding interface shares the same LLM facade as chat. from republic import LLM llm = LLM(model=\"openrouter:openai/text-embedding-3-small\", api_key=\"<API_KEY>\") out = llm.embed([\"republic\", \"tape-first\"]) if out.error: print(out.error.kind, out.error.message) else: print(out.value) You can also override the model per call: out = llm.embed( \"incident root cause analysis\", model=\"openrouter:openai/text-embedding-3-small\", )","title":"Embeddings"},{"location":"guides/embeddings/#embeddings","text":"The embedding interface shares the same LLM facade as chat. from republic import LLM llm = LLM(model=\"openrouter:openai/text-embedding-3-small\", api_key=\"<API_KEY>\") out = llm.embed([\"republic\", \"tape-first\"]) if out.error: print(out.error.kind, out.error.message) else: print(out.value) You can also override the model per call: out = llm.embed( \"incident root cause analysis\", model=\"openrouter:openai/text-embedding-3-small\", )","title":"Embeddings"},{"location":"guides/stream-events/","text":"Stream Events \u00b6 Republic provides two streaming modes: stream(...) : text deltas only. stream_events(...) : full events including text, tools, usage, and final. Text Stream \u00b6 from republic import LLM llm = LLM(model=\"openrouter:openai/gpt-4o-mini\", api_key=\"<API_KEY>\") stream = llm.stream(\"Give me three words.\") text = \"\".join(chunk for chunk in stream) print(text) print(stream.error) print(stream.usage) Event Stream \u00b6 events = llm.stream_events( \"Plan deployment and call a tool if needed.\", tools=[], ) for event in events: if event.kind == \"text\": print(event.data[\"delta\"], end=\"\") elif event.kind == \"tool_call\": print(\"\\ncall=\", event.data) elif event.kind == \"tool_result\": print(\"\\nresult=\", event.data) elif event.kind == \"usage\": print(\"\\nusage=\", event.data) elif event.kind == \"final\": print(\"\\nfinal=\", event.data) The final event contains text/tool_calls/tool_results/usage/ok , which is a good fit for final UI state or audit persistence.","title":"Stream Events"},{"location":"guides/stream-events/#stream-events","text":"Republic provides two streaming modes: stream(...) : text deltas only. stream_events(...) : full events including text, tools, usage, and final.","title":"Stream Events"},{"location":"guides/stream-events/#text-stream","text":"from republic import LLM llm = LLM(model=\"openrouter:openai/gpt-4o-mini\", api_key=\"<API_KEY>\") stream = llm.stream(\"Give me three words.\") text = \"\".join(chunk for chunk in stream) print(text) print(stream.error) print(stream.usage)","title":"Text Stream"},{"location":"guides/stream-events/#event-stream","text":"events = llm.stream_events( \"Plan deployment and call a tool if needed.\", tools=[], ) for event in events: if event.kind == \"text\": print(event.data[\"delta\"], end=\"\") elif event.kind == \"tool_call\": print(\"\\ncall=\", event.data) elif event.kind == \"tool_result\": print(\"\\nresult=\", event.data) elif event.kind == \"usage\": print(\"\\nusage=\", event.data) elif event.kind == \"final\": print(\"\\nfinal=\", event.data) The final event contains text/tool_calls/tool_results/usage/ok , which is a good fit for final UI state or audit persistence.","title":"Event Stream"},{"location":"guides/tape/","text":"Tape \u00b6 Tape is an append-only execution log and a context selector. Core Actions \u00b6 handoff(name, state=...) : Create a new task anchor. chat(...) : Continue on the current tape and record the run. query.all() : Read all entries (message/tool/error/event). query.*() : Run slice queries. Deprecation \u00b6 read_entries() is deprecated. Use tape.query.all() instead. # before (deprecated) entries = tape.read_entries() # after entries = list(tape.query.all()) Minimal Session \u00b6 from republic import LLM llm = LLM(model=\"openrouter:openrouter/free\", api_key=\"<API_KEY>\") tape = llm.tape(\"ops\") tape.handoff(\"incident_42\", state={\"owner\": \"tier1\"}) out = tape.chat(\"Connection pool is exhausted. Give triage steps.\", max_tokens=96) print(out) print([entry.kind for entry in tape.query.all()]) Anchor-Based Context Slicing \u00b6 tape.handoff(\"incident_43\") _ = tape.chat(\"This time the issue is cache penetration.\") previous = tape.query.after_anchor(\"incident_42\").all() print([entry.kind for entry in previous]) Conventions \u00b6 Tape entries are append-only and never overwrite history. Query/Context depend on entry order, not external indexes. Errors are recorded as first-class entries for replay. Async Tape Store \u00b6 When tape_store is configured as an AsyncTapeStore (or its adapter), async calls with tape=... read and write context through AsyncTapeManager . from republic import LLM, TapeContext from republic.tape.store import AsyncTapeStoreAdapter, InMemoryTapeStore llm = LLM( model=\"openai:gpt-4o-mini\", api_key=\"<API_KEY>\", tape_store=AsyncTapeStoreAdapter(InMemoryTapeStore()), context=TapeContext(anchor=None), ) first = await llm.chat_async(\"Investigate DB timeout\", tape=\"ops\") second = await llm.chat_async(\"Include rollback criteria\", tape=\"ops\") print(first, second) Sync vs Async Rules \u00b6 When tape_store is an AsyncTapeStore : Sync APIs with tape=... are unavailable (they raise ErrorPayload ). Use async APIs instead: chat_async , tool_calls_async , run_tools_async , stream_async , and stream_events_async . llm.tape(\"...\") returns a session object that exposes both sync and async methods; in this mode, use the *_async methods.","title":"Tape"},{"location":"guides/tape/#tape","text":"Tape is an append-only execution log and a context selector.","title":"Tape"},{"location":"guides/tape/#core-actions","text":"handoff(name, state=...) : Create a new task anchor. chat(...) : Continue on the current tape and record the run. query.all() : Read all entries (message/tool/error/event). query.*() : Run slice queries.","title":"Core Actions"},{"location":"guides/tape/#deprecation","text":"read_entries() is deprecated. Use tape.query.all() instead. # before (deprecated) entries = tape.read_entries() # after entries = list(tape.query.all())","title":"Deprecation"},{"location":"guides/tape/#minimal-session","text":"from republic import LLM llm = LLM(model=\"openrouter:openrouter/free\", api_key=\"<API_KEY>\") tape = llm.tape(\"ops\") tape.handoff(\"incident_42\", state={\"owner\": \"tier1\"}) out = tape.chat(\"Connection pool is exhausted. Give triage steps.\", max_tokens=96) print(out) print([entry.kind for entry in tape.query.all()])","title":"Minimal Session"},{"location":"guides/tape/#anchor-based-context-slicing","text":"tape.handoff(\"incident_43\") _ = tape.chat(\"This time the issue is cache penetration.\") previous = tape.query.after_anchor(\"incident_42\").all() print([entry.kind for entry in previous])","title":"Anchor-Based Context Slicing"},{"location":"guides/tape/#conventions","text":"Tape entries are append-only and never overwrite history. Query/Context depend on entry order, not external indexes. Errors are recorded as first-class entries for replay.","title":"Conventions"},{"location":"guides/tape/#async-tape-store","text":"When tape_store is configured as an AsyncTapeStore (or its adapter), async calls with tape=... read and write context through AsyncTapeManager . from republic import LLM, TapeContext from republic.tape.store import AsyncTapeStoreAdapter, InMemoryTapeStore llm = LLM( model=\"openai:gpt-4o-mini\", api_key=\"<API_KEY>\", tape_store=AsyncTapeStoreAdapter(InMemoryTapeStore()), context=TapeContext(anchor=None), ) first = await llm.chat_async(\"Investigate DB timeout\", tape=\"ops\") second = await llm.chat_async(\"Include rollback criteria\", tape=\"ops\") print(first, second)","title":"Async Tape Store"},{"location":"guides/tape/#sync-vs-async-rules","text":"When tape_store is an AsyncTapeStore : Sync APIs with tape=... are unavailable (they raise ErrorPayload ). Use async APIs instead: chat_async , tool_calls_async , run_tools_async , stream_async , and stream_events_async . llm.tape(\"...\") returns a session object that exposes both sync and async methods; in this mode, use the *_async methods.","title":"Sync vs Async Rules"},{"location":"guides/text/","text":"Text Decisions \u00b6 if_ and classify are useful when you want model decisions in a clear structured form. if_ \u00b6 from republic import LLM llm = LLM(model=\"openrouter:openai/gpt-4o-mini\", api_key=\"<API_KEY>\") decision = llm.if_(\"The release is blocked by a migration failure.\", \"Should we page on-call now?\") print(decision.value) # bool | None print(decision.error) classify \u00b6 label = llm.classify( \"User asks for invoice and tax receipt.\", [\"sales\", \"support\", \"finance\"], ) print(label.value) # one of choices | None print(label.error) Usage Tips \u00b6 Treat these as shortcut entry points for agentic if and classification. Keep business logic in regular Python branches for testability and audits.","title":"Text Decisions"},{"location":"guides/text/#text-decisions","text":"if_ and classify are useful when you want model decisions in a clear structured form.","title":"Text Decisions"},{"location":"guides/text/#if_","text":"from republic import LLM llm = LLM(model=\"openrouter:openai/gpt-4o-mini\", api_key=\"<API_KEY>\") decision = llm.if_(\"The release is blocked by a migration failure.\", \"Should we page on-call now?\") print(decision.value) # bool | None print(decision.error)","title":"if_"},{"location":"guides/text/#classify","text":"label = llm.classify( \"User asks for invoice and tax receipt.\", [\"sales\", \"support\", \"finance\"], ) print(label.value) # one of choices | None print(label.error)","title":"classify"},{"location":"guides/text/#usage-tips","text":"Treat these as shortcut entry points for agentic if and classification. Keep business logic in regular Python branches for testability and audits.","title":"Usage Tips"},{"location":"guides/tools/","text":"Tools \u00b6 Tool workflows have two paths: Automatic execution: llm.run_tools(...) Manual execution: llm.tool_calls(...) + llm.tools.execute(...) Define a Tool \u00b6 from republic import tool @tool def get_weather(city: str) -> str: return f\"{city}: sunny\" Automatic Execution (Faster) \u00b6 from republic import LLM llm = LLM(model=\"openrouter:openai/gpt-4o-mini\", api_key=\"<API_KEY>\") out = llm.run_tools(\"What is weather in Tokyo?\", tools=[get_weather]) print(out.kind) # text | tools | error print(out.tool_results) print(out.error) Manual Execution (More Control) \u00b6 calls = llm.tool_calls(\"Use get_weather for Berlin.\", tools=[get_weather]) if calls.error: raise RuntimeError(calls.error.message) execution = llm.tools.execute(calls.value, tools=[get_weather]) print(execution.tool_results) print(execution.error) Tools with Context \u00b6 When a tool needs tape/run metadata, declare context=True . from republic import ToolContext, tool @tool(context=True) def save_ticket(title: str, context: ToolContext) -> dict[str, str]: return { \"title\": title, \"run_id\": context.run_id, \"tape\": context.tape or \"none\", }","title":"Tools"},{"location":"guides/tools/#tools","text":"Tool workflows have two paths: Automatic execution: llm.run_tools(...) Manual execution: llm.tool_calls(...) + llm.tools.execute(...)","title":"Tools"},{"location":"guides/tools/#define-a-tool","text":"from republic import tool @tool def get_weather(city: str) -> str: return f\"{city}: sunny\"","title":"Define a Tool"},{"location":"guides/tools/#automatic-execution-faster","text":"from republic import LLM llm = LLM(model=\"openrouter:openai/gpt-4o-mini\", api_key=\"<API_KEY>\") out = llm.run_tools(\"What is weather in Tokyo?\", tools=[get_weather]) print(out.kind) # text | tools | error print(out.tool_results) print(out.error)","title":"Automatic Execution (Faster)"},{"location":"guides/tools/#manual-execution-more-control","text":"calls = llm.tool_calls(\"Use get_weather for Berlin.\", tools=[get_weather]) if calls.error: raise RuntimeError(calls.error.message) execution = llm.tools.execute(calls.value, tools=[get_weather]) print(execution.tool_results) print(execution.error)","title":"Manual Execution (More Control)"},{"location":"guides/tools/#tools-with-context","text":"When a tool needs tape/run metadata, declare context=True . from republic import ToolContext, tool @tool(context=True) def save_ticket(title: str, context: ToolContext) -> dict[str, str]: return { \"title\": title, \"run_id\": context.run_id, \"tape\": context.tape or \"none\", }","title":"Tools with Context"}]}